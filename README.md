# API for Open LLMs

å¼€æºå¤§æ¨¡å‹çš„ç»Ÿä¸€åç«¯æ¥å£ï¼Œä¸ `OpenAI` çš„å“åº”ä¿æŒä¸€è‡´

# ğŸ¼ æ¨¡å‹

æ”¯æŒå¤šç§å¼€æºå¤§æ¨¡å‹

+ âœ… [ChatGLM](https://github.com/THUDM/ChatGLM-6B)

+ âœ… [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)

+ âœ… [Phoenix](https://github.com/FreedomIntelligence/LLMZoo)

+ âœ… [MOSS](https://github.com/OpenLMLab/MOSS)

# ğŸ³ ç¯å¢ƒé…ç½®

## dockerå¯åŠ¨ï¼ˆ**æ¨è**ï¼‰

æ„å»ºé•œåƒ

```shell
docker build -t llm-api:pytorch .
```

å¯åŠ¨å®¹å™¨

```shell
docker run -it -d --gpus all --ipc=host --net=host -p 80:80 --name=chatglm \
    --ulimit memlock=-1 --ulimit stack=67108864 \
    -v `pwd`:/workspace \
    llm-api:pytorch \
    python api/app.py \
    --port 80 \
    --allow-credentials \
    --model_name chatglm \
    --model_path THUDM/chatglm-6b \
    --embedding_name GanymedeNil/text2vec-large-chinese
```

ä¸»è¦å‚æ•°å«ä¹‰ï¼š

+ `model_name`: æ¨¡å‹åç§°ï¼Œå¦‚`chatglm`ã€`phoenix`ã€`moss`ç­‰

+ `model_path`: å¼€æºå¤§æ¨¡å‹çš„æ–‡ä»¶æ‰€åœ¨è·¯å¾„

+ `embedding_name`ï¼ˆå¯é€‰é¡¹ï¼‰: åµŒå…¥æ¨¡å‹çš„æ–‡ä»¶æ‰€åœ¨è·¯å¾„

## æœ¬åœ°å¯åŠ¨

å®‰è£… `pytorch` ç¯å¢ƒ

```shell
conda create -n pytorch python=3.8
conda activate pytorch
conda install pytorch cudatoolkit -c pytorch
```

å®‰è£…ä¾èµ–åŒ…

```shell
pip install -r requirements.txt
```

å¯åŠ¨åç«¯

```shell
python api/app.py \
    --port 80 \
    --allow-credentials \
    --model_path THUDM/chatglm-6b \
    --embedding_name GanymedeNil/text2vec-large-chinese
```

# ğŸ¤– ä½¿ç”¨æ–¹å¼

## ç¯å¢ƒå˜é‡

+ `OPENAI_API_KEY`: æ­¤å¤„éšæ„å¡«ä¸€ä¸ªå­—ç¬¦ä¸²å³å¯

+ `OPENAI_API_BASE`: åç«¯å¯åŠ¨çš„æ¥å£åœ°å€ï¼Œå¦‚ï¼šhttp://192.168.0.xx:80/v1

## [å‘½ä»¤ç«¯å¯åŠ¨å¤šè½®å¯¹è¯](applications/chat/client.py)

```shell
cd applications/chat

python client.py --model_name chatglm
```

## [openai-python](https://github.com/openai/openai-python)

### Chat Completions

```python
import openai

openai.api_base = "http://192.168.0.xx:80/v1"

# Enter any non-empty API key to pass the client library's check.
openai.api_key = "xxx"

# Enter any non-empty model name to pass the client library's check.
completion = openai.ChatCompletion.create(
    model="chatglm-6b",
    messages=[
        {"role": "user", "content": "ä½ å¥½"},
    ],
    stream=False,
)

print(completion.choices[0].message.content)
# ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
```

### Completions

```python
import openai

openai.api_base = "http://192.168.0.xx:80/v1"

# Enter any non-empty API key to pass the client library's check.
openai.api_key = "xxx"

# Enter any non-empty model name to pass the client library's check.
completion = openai.Completion.create(prompt="ä½ å¥½", model="chatglm-6b")

print(completion.choices[0].text)
# ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
```

### Embeddings

```python
import openai

openai.api_base = "http://192.168.0.xx:80/v1"

# Enter any non-empty API key to pass the client library's check.
openai.api_key = "xxx"

# compute the embedding of the text
embedding = openai.Embedding.create(
    input="ä»€ä¹ˆæ˜¯chatgptï¼Ÿ", 
    model="text2vec-large-chinese"
)

print(embedding['data'][0]['embedding'])
```

## [langchain](https://github.com/hwchase17/langchain)

### Chat Completions

```python
import os

os.environ["OPENAI_API_BASE"] = "http://192.168.0.xx:80/v1"
os.environ["OPENAI_API_KEY"] = "xxx"

from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

chat = ChatOpenAI()
print(chat([HumanMessage(content="ä½ å¥½")]))
# content='ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚' additional_kwargs={}
```

### Completions

```python
import os

os.environ["OPENAI_API_BASE"] = "http://192.168.0.xx:80/v1"
os.environ["OPENAI_API_KEY"] = "xxx"

from langchain.llms import OpenAI

llm = OpenAI()
print(llm("ä½ å¥½"))
# ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
```

### Embeddings

```python
import os

os.environ["OPENAI_API_BASE"] = "http://192.168.0.xx:80/v1"
os.environ["OPENAI_API_KEY"] = "xxx"

from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
query_result = embeddings.embed_query("ä»€ä¹ˆæ˜¯chatgptï¼Ÿ")
print(query_result)
```

## å¯æ¥å…¥çš„é¡¹ç›®

**é€šè¿‡ä¿®æ”¹ä¸Šé¢çš„ `OPENAI_API_BASE` ç¯å¢ƒå˜é‡ï¼Œå¤§éƒ¨åˆ†çš„ `chatgpt` åº”ç”¨å’Œå‰åç«¯é¡¹ç›®éƒ½å¯ä»¥æ— ç¼è¡”æ¥ï¼**

+ [ChatGPT-Next-Web](https://github.com/Yidadaa/ChatGPT-Next-Web)

```shell
docker run -d -p 3000:3000 \
   -e OPENAI_API_KEY="sk-xxxx" \
   -e BASE_URL="http://192.168.0.xx:80" \
   yidadaa/chatgpt-next-web
```

![web](images/web.png)

+ [dify](https://github.com/langgenius/dify)

![dify](images/dify.png)


