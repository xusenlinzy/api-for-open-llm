# API for Open LLMs

å¼€æºå¤§æ¨¡å‹çš„ç»Ÿä¸€åç«¯æ¥å£ï¼Œä¸ `OpenAI` çš„å“åº”ä¿æŒä¸€è‡´

# ğŸ¼ æ¨¡å‹

æ”¯æŒå¤šç§å¼€æºå¤§æ¨¡å‹

+ [ChatGLM](https://github.com/THUDM/ChatGLM-6B)

+ [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)

+ [Phoenix](https://github.com/FreedomIntelligence/LLMZoo)

+ [MOSS](https://github.com/OpenLMLab/MOSS)

# ğŸ³ ç¯å¢ƒé…ç½®

## 1. dockerå¯åŠ¨ï¼ˆ**æ¨è**ï¼‰

æ„å»ºé•œåƒ

```shell
docker build -t llm-api:pytorch .
```

å¯åŠ¨å®¹å™¨

```shell
docker run -it -d --gpus all --ipc=host --net=host -p 80:80 --name=chatglm \
    --ulimit memlock=-1 --ulimit stack=67108864 \
    -v `pwd`:/workspace \
    llm-api:pytorch \
    python main.py \
    --port 80 \
    --allow-credentials \
    --model_name chatglm \
    --model_path THUDM/chatglm-6b \
    --embedding_name GanymedeNil/text2vec-large-chinese
```

ä¸»è¦å‚æ•°å«ä¹‰ï¼š

+ `model_name`: æ¨¡å‹åç§°ï¼Œå¦‚`chatglm`ã€`phoenix`ã€`moss`ç­‰

+ `model_path`: å¼€æºå¤§æ¨¡å‹çš„æ–‡ä»¶æ‰€åœ¨è·¯å¾„

+ `embedding_name`: åµŒå…¥æ¨¡å‹çš„æ–‡ä»¶æ‰€åœ¨è·¯å¾„

## 2. æœ¬åœ°å¯åŠ¨

å®‰è£… `pytorch` ç¯å¢ƒ

```shell
conda create -n pytorch python=3.8
conda activate pytorch
conda install pytorch cudatoolkit -c pytorch
```

å®‰è£…ä¾èµ–åŒ…

```shell
pip install -r requirements.txt
```

å¯åŠ¨åç«¯

```shell
python main.py \
    --port 80 \
    --allow-credentials \
    --model_path THUDM/chatglm-6b \
    --embedding_name GanymedeNil/text2vec-large-chinese
```

# ğŸ¤– ä½¿ç”¨æ–¹å¼

## 1. ç¯å¢ƒå˜é‡

+ `OPENAI_API_KEY`: æ­¤å¤„éšæ„å¡«ä¸€ä¸ªå­—ç¬¦ä¸²å³å¯

+ `OPENAI_API_BASE`: åç«¯å¯åŠ¨çš„æ¥å£åœ°å€ï¼Œå¦‚ï¼šhttp://192.168.0.xx:80/v1


## 2. [openai-python](https://github.com/openai/openai-python)

```python
import openai

# Point requests to Basaran by overwriting openai.api_base.
# Or you can use the OPENAI_API_BASE environment variable instead.
openai.api_base = "http://192.168.0.xx:80/v1"

# Enter any non-empty API key to pass the client library's check.
openai.api_key = "xxx"

# Enter any non-empty model name to pass the client library's check.
completion = openai.ChatCompletion.create(
    model="chatglm-6b",
    messages=[
        {"role": "user", "content": "ä½ å¥½"},
    ],
    stream=False,
)

print(completion.choices[0].message.content)
# ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
```

## 3. [langchain](https://github.com/hwchase17/langchain)

```python
import os

os.environ["OPENAI_API_BASE"] = "http://192.168.0.xx:80/v1"
os.environ["OPENAI_API_KEY"] = "xxx"

from langchain.chat_models import ChatOpenAI
from langchain.schema import (
    HumanMessage
)

chat = ChatOpenAI()
print(chat([HumanMessage(content="ä½ å¥½")]))
# content='ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ' additional_kwargs={}
```

## 4. å¯æ¥å…¥çš„é¡¹ç›®

**é€šè¿‡ä¿®æ”¹ä¸Šé¢çš„ `OPENAI_API_BASE` ç¯å¢ƒå˜é‡ï¼Œå¤§éƒ¨åˆ†çš„ `chatgpt` åº”ç”¨å’Œå‰åç«¯é¡¹ç›®éƒ½å¯ä»¥æ— ç¼è¡”æ¥ï¼**

+ [ChatGPT-Next-Web](https://github.com/Yidadaa/ChatGPT-Next-Web)

```shell
docker run -d -p 3000:3000 \
   -e OPENAI_API_KEY="sk-xxxx" \
   -e BASE_URL="http://192.168.0.xx:80" \
   yidadaa/chatgpt-next-web
```

![web](images/web.png)

+ [dify](https://github.com/langgenius/dify)

![dify](images/dify.png)


